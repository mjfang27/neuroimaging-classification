{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjfang/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE project directory is defined as -f\n",
      "Parsing 111\n",
      "Training voxels...\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjfang/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:158: DeprecationWarning: get_affine method is deprecated.\n",
      "Please use the ``img.affine`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n",
      "/home/mjfang/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:159: DeprecationWarning: get_affine method is deprecated.\n",
      "Please use the ``img.affine`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n",
      "/home/mjfang/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:168: DeprecationWarning: get_affine method is deprecated.\n",
      "Please use the ``img.affine`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n",
      "/home/mjfang/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:169: DeprecationWarning: get_affine method is deprecated.\n",
      "Please use the ``img.affine`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n",
      "Parsing 3129\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "permutation (third) step in forward modeling analysis - building a forward model, testing with 2 images held out. This is the submission script to run the analysis, submitting to a SLURM cluster. You MUST edit the last line of this script for your particular submission command. If you do not use this kind of cluster, you should edit the end of the script (where submission occurs) to fit your format.\n",
    "\n",
    "  Classification framework\n",
    "  for image1 in all images:\n",
    "     for image2 in allimages:\n",
    "         if image1 != image2:\n",
    "             hold out image 1 and image 2, generate regression parameter matrix using other images\n",
    "             generate predicted image for image 1 [PR1]\n",
    "             generate predicted image for image 2 [PR2]\n",
    "             classify image 1 as fitting best to PR1 or PR2\n",
    "             classify image 2 as fitting best to PR1 or PR2\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import sys\n",
    "\n",
    "from pybraincompare.compare.maths import calculate_correlation\n",
    "from pybraincompare.compare.mrutils import get_images_df\n",
    "from pybraincompare.mr.datasets import get_standard_mask\n",
    "from pybraincompare.mr.transformation import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "import nibabel\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "from utils import (\n",
    "   get_base, get_pwd, make_dirs\n",
    ")\n",
    "\n",
    "# VARIABLES FOR SLURM\n",
    "max_runtime=\"2-00:00\"                    # Two days. Each script needs ~10-15 minutes, 30 is recommended for buffer\n",
    "memory=\"32000\"                           # 16000 might also work\n",
    "submission_command=\"sbatch\"                  # Your cluster submission command, eg sbatch, qsub\n",
    "submission_args=\"-p russpold --qos russpold\" # Does not need spaces to left and right\n",
    "\n",
    "\n",
    "# Get the base and present working directory\n",
    "base = get_base()\n",
    "here = get_pwd()\n",
    "\n",
    "data = os.path.abspath(\"%s/data\" %(here))\n",
    "results = os.path.abspath(\"%s/results\" %(here))\n",
    "output_folder = \"%s/permutations\" %results  \n",
    "\n",
    "# Make the output directory\n",
    "make_dirs(output_folder,reason=\"for permutation results.\")\n",
    "\n",
    "# Images by Concepts data frame\n",
    "labels_tsv = \"%s/concepts_binary_df.tsv\" %results\n",
    "images = pandas.read_csv(labels_tsv,sep=\"\\t\",index_col=0)\n",
    "image_lookup = \"%s/image_nii_lookup.pkl\" %results\n",
    "\n",
    "# We will need these folders to exist for job and output files\n",
    "log_folders = [\"%s/.out\" %here,\"%s/.job\" %here]\n",
    "make_dirs(log_folders)    \n",
    "\n",
    "# Image metadata with number of subjects included\n",
    "contrast_file = \"%s/filtered_contrast_images.tsv\" %results\n",
    "done = False\n",
    "for image1_holdout in images.index.tolist():\n",
    "    print \"Parsing %s\" %(image1_holdout)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    for image2_holdout in images.index.tolist():\n",
    "        if (image1_holdout != image2_holdout) and (image1_holdout < image2_holdout):\n",
    "            output_file = \"%s/%s_%s_perform.pkl\" %(output_folder,image1_holdout,image2_holdout)\n",
    "            if not os.path.exists(output_file):\n",
    "\n",
    "                # Images by Concept data frame, our X\n",
    "                X = pandas.read_csv(labels_tsv,sep=\"\\t\",index_col=0)\n",
    "\n",
    "                # Images data frame with contrast info, and importantly, number of subjects\n",
    "                image_df = pandas.read_csv(contrast_file,sep=\"\\t\",index_col=0)\n",
    "                image_df.index = image_df.image_id\n",
    "\n",
    "                # We should standardize cognitive concepts before modeling\n",
    "                # http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "                scaled = pandas.DataFrame(StandardScaler().fit_transform(X))\n",
    "                scaled.columns = X.columns\n",
    "                scaled.index = X.index\n",
    "                X = scaled\n",
    "\n",
    "                # Dictionary to look up image files (4mm)\n",
    "                lookup = pickle.load(open(image_lookup,\"rb\"))\n",
    "\n",
    "                # Get standard mask, 4mm\n",
    "                standard_mask=get_standard_mask(4)\n",
    "\n",
    "                # We will save data to dictionary\n",
    "                result = dict()\n",
    "\n",
    "                concepts = X.columns.tolist()\n",
    "\n",
    "                # We will go through each voxel (column) in a data frame of image data\n",
    "                image_paths = lookup.values()\n",
    "                mr = get_images_df(file_paths=image_paths,mask=standard_mask)\n",
    "                image_ids = [int(os.path.basename(x).split(\".\")[0]) for x in image_paths]\n",
    "                mr.index = image_ids\n",
    "\n",
    "                norm = pandas.DataFrame(columns=mr.columns)\n",
    "\n",
    "                # Normalize the image data by number of subjects\n",
    "                #V* = V/sqrt(S) \n",
    "                for row in mr.iterrows():\n",
    "                    subid = row[0]\n",
    "                    number_of_subjects = image_df.loc[subid].number_of_subjects.tolist()\n",
    "                    norm_vector = row[1]/numpy.sqrt(number_of_subjects)\n",
    "                    norm.loc[subid] = norm_vector\n",
    "\n",
    "                del mr\n",
    "\n",
    "                # Get the labels for holdout images\n",
    "                holdout1Y = X.loc[image1_holdout,:]\n",
    "                holdout2Y = X.loc[image2_holdout,:]\n",
    "\n",
    "                # what we can do is generate a predicted image for a particular set of concepts (e.g, for a left out image) by simply multiplying the concept vector by the regression parameters at each voxel.  then you can do the mitchell trick of asking whether you can accurately classify two left-out images by matching them with the two predicted images. \n",
    "\n",
    "                regression_params = pandas.DataFrame(0,index=norm.columns,columns=concepts)\n",
    "                intercept_params = pandas.DataFrame(0,index=norm.columns,columns=[\"intercept\"])\n",
    "\n",
    "                print \"Training voxels...\"\n",
    "                for voxel in norm.columns:\n",
    "                    train = [x for x in X.index if x not in [image1_holdout,image2_holdout] and x in norm.index]\n",
    "                    Y = norm.loc[train,voxel].tolist()\n",
    "                    Xtrain = X.loc[train,:] \n",
    "                    # Use regularized regression\n",
    "                    clf = linear_model.ElasticNet(alpha=0.1)\n",
    "                    clf.fit(Xtrain,Y)\n",
    "                    regression_params.loc[voxel,:] = clf.coef_.tolist()\n",
    "                    intercept_params.loc[voxel,\"intercept\"] = clf.intercept_    \n",
    "\n",
    "                print \"Making predictions...\"\n",
    "                # Use regression parameters to generate predicted images\n",
    "                # data * .coef_ + intercept_\n",
    "                concept_vector1 =  pandas.DataFrame(holdout1Y)\n",
    "                concept_vector2 =  pandas.DataFrame(holdout2Y)\n",
    "                predicted_nii1 =  regression_params.dot(concept_vector1)[image1_holdout] + intercept_params[\"intercept\"] \n",
    "                predicted_nii2 =  regression_params.dot(concept_vector2)[image2_holdout] + intercept_params[\"intercept\"]\n",
    "\n",
    "\n",
    "                # Turn into nifti images\n",
    "                nii1 = numpy.zeros(standard_mask.shape)\n",
    "                nii2 = numpy.zeros(standard_mask.shape)\n",
    "                nii1[standard_mask.get_data()!=0] = predicted_nii1.tolist()\n",
    "                nii2[standard_mask.get_data()!=0] = predicted_nii2.tolist()\n",
    "                nii1 = nibabel.Nifti1Image(nii1,affine=standard_mask.get_affine())\n",
    "                nii2 = nibabel.Nifti1Image(nii2,affine=standard_mask.get_affine())\n",
    "\n",
    "                # Turn the holdout image data back into nifti\n",
    "                actual1 = norm.loc[image1_holdout,:]\n",
    "                actual2 = norm.loc[image2_holdout,:]\n",
    "                actual_nii1 = numpy.zeros(standard_mask.shape)\n",
    "                actual_nii2 = numpy.zeros(standard_mask.shape)\n",
    "                actual_nii1[standard_mask.get_data()!=0] = actual1.tolist()\n",
    "                actual_nii2[standard_mask.get_data()!=0] = actual2.tolist()\n",
    "                actual_nii1 = nibabel.Nifti1Image(actual_nii1,affine=standard_mask.get_affine())\n",
    "                actual_nii2 = nibabel.Nifti1Image(actual_nii2,affine=standard_mask.get_affine())\n",
    "\n",
    "                # Make a dictionary to lookup images based on nifti\n",
    "                lookup = dict()\n",
    "                lookup[actual_nii1] = image1_holdout\n",
    "                lookup[actual_nii2] = image2_holdout\n",
    "                lookup[nii1] = image1_holdout\n",
    "                lookup[nii2] = image2_holdout\n",
    "\n",
    "                comparison_df = pandas.DataFrame(columns=[\"actual\",\"predicted\",\"cca_score\"])\n",
    "                comparisons = [[actual_nii1,nii1],[actual_nii1,nii2],[actual_nii2,nii1],[actual_nii2,nii2]]\n",
    "                count=0\n",
    "                for comp in comparisons:\n",
    "                    name1 = lookup[comp[0]]\n",
    "                    name2 = lookup[comp[1]]\n",
    "                    corr = calculate_correlation(comp,mask=standard_mask)\n",
    "                    comparison_df.loc[count] = [name1,name2,corr] \n",
    "                    count+=1\n",
    "\n",
    "                #   actual  predicted  cca_score\n",
    "                #0    3186       3186   0.908997\n",
    "                #1    3186        420   0.485644\n",
    "                #2     420       3186   0.044668\n",
    "                #3     420        420   0.657109\n",
    "\n",
    "                result[\"comparison_df\"] = comparison_df\n",
    "\n",
    "                # Calculate accuracy\n",
    "                correct = 0\n",
    "                acc1 = comparison_df[comparison_df.actual==image1_holdout]\n",
    "                acc2 = comparison_df[comparison_df.actual==image2_holdout]\n",
    "\n",
    "                # Did we predict image1 to be image1?\n",
    "                if acc1.loc[acc1.predicted==image1_holdout,\"cca_score\"].tolist()[0] > acc1.loc[acc1.predicted==image2_holdout,\"cca_score\"].tolist()[0]:\n",
    "                    correct+=1\n",
    "\n",
    "                # Did we predict image2 to be image2?\n",
    "                if acc2.loc[acc2.predicted==image2_holdout,\"cca_score\"].tolist()[0] > acc2.loc[acc2.predicted==image1_holdout,\"cca_score\"].tolist()[0]:\n",
    "                    correct+=1\n",
    "\n",
    "                result[\"number_correct\"] = correct\n",
    "\n",
    "                # We should have a measure of encoding regression performance (in addition to classification). I like out of sample R^2 i.e. how much variance is explained by the model using only cognitive concepts.\n",
    "                result[\"r2_%s\" %(image1_holdout)] = r2_score(actual1, predicted_nii1)\n",
    "                result[\"r2_%s\" %(image2_holdout)] = r2_score(actual2, predicted_nii2)\n",
    "\n",
    "                pickle.dump(result,open(output_file,\"wb\"))\n",
    "                print \"complete!\"\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "                \n",
    "                \n",
    "#                 job_id = \"%s_%s\" %(image1_holdout,image2_holdout)\n",
    "#                 filey = \"%s/.job/class_%s.job\" %(here,job_id)\n",
    "#                 filey = open(filey,\"w\")\n",
    "#                 filey.writelines(\"#!/bin/bash\\n\")\n",
    "#                 filey.writelines(\"#SBATCH --job-name=%s\\n\" %(job_id))\n",
    "#                 filey.writelines(\"#SBATCH --output=%s/.out/%s.out\\n\" %(here,job_id))\n",
    "#                 filey.writelines(\"#SBATCH --error=%s/.out/%s.err\\n\" %(here,job_id))\n",
    "#                 filey.writelines(\"#SBATCH --time=%s\\n\" %(max_runtime))\n",
    "#                 filey.writelines(\"#SBATCH --mem=%s\\n\" %(memory))\n",
    "#                 filey.writelines(\"python %s/2.encoding_regression_performance.py %s %s %s %s %s %s\" %(here, \n",
    "#                                                                                                       image1_holdout, \n",
    "#                                                                                                       image2_holdout, \n",
    "#                                                                                                       output_file, \n",
    "#                                                                                                       labels_tsv, \n",
    "#                                                                                                       image_lookup, \n",
    "#                                                                                                       contrast_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.011747\n",
       "1        0.039312\n",
       "2        0.078127\n",
       "3        0.014027\n",
       "4        0.043399\n",
       "5        0.059229\n",
       "6       -0.031192\n",
       "7        0.028993\n",
       "8        0.041878\n",
       "9        0.017269\n",
       "10      -0.036731\n",
       "11       0.037625\n",
       "12       0.058208\n",
       "13       0.024432\n",
       "14      -0.020017\n",
       "15      -0.063942\n",
       "16      -0.048427\n",
       "17      -0.020679\n",
       "18      -0.000135\n",
       "19       0.020998\n",
       "20      -0.016895\n",
       "21      -0.054993\n",
       "22      -0.037133\n",
       "23      -0.037534\n",
       "24      -0.035869\n",
       "25       0.021836\n",
       "26       0.035510\n",
       "27       0.024194\n",
       "28       0.018264\n",
       "29       0.037066\n",
       "           ...   \n",
       "28519    0.000500\n",
       "28520    0.027448\n",
       "28521    0.045577\n",
       "28522    0.017893\n",
       "28523   -0.002870\n",
       "28524   -0.024356\n",
       "28525   -0.037735\n",
       "28526    0.015849\n",
       "28527   -0.014313\n",
       "28528   -0.020697\n",
       "28529    0.001022\n",
       "28530   -0.001267\n",
       "28531   -0.000196\n",
       "28532   -0.022467\n",
       "28533   -0.059719\n",
       "28534   -0.075053\n",
       "28535   -0.079916\n",
       "28536   -0.029295\n",
       "28537    0.009677\n",
       "28538    0.018777\n",
       "28539   -0.011627\n",
       "28540   -0.024022\n",
       "28541   -0.070966\n",
       "28542   -0.095829\n",
       "28543   -0.028345\n",
       "28544    0.007951\n",
       "28545    0.005370\n",
       "28546   -0.020523\n",
       "28547   -0.067230\n",
       "28548   -0.076357\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_nii1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
